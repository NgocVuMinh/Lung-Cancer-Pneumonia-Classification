{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The application of deep learning in lung cancerous lesion detection (example notebook)\n",
    "**Contributors: Ngoc M. Vu, Phuong T.M. Chu, Tram P.B. Ha**\n",
    "\n",
    "This notebook can be used to train an InceptionV3 model for the classification of lung cancer and non-cancer pneumonia-only using chest CT scans.\n",
    "\n",
    "Details about the data and model configurations can be found at:\n",
    "\n",
    "Overview of the classification models:\n",
    "<p float=\"left\">\n",
    "  <img src=\"https://github.com/NgocVuMinh/Lung-Cancer-Pneumonia-Classification/blob/main/overview1.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "from skimage import io\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.applications import InceptionV3 # ResNet50, ResNet101, VGG16, VGG19, MobileNetV2, InceptionResNetV2, Xception\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.src.preprocessing.image import ImageDataGenerator\n",
    "from keras.src.preprocessing import image\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 99\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "val_dir = '/home/ngoc/lc/sample_data/Validation'\n",
    "train_dir = '/home/ngoc/lc/sample_data/Training'\n",
    "test_dir = '/home/ngoc/lc/sample_data/Testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_types=['Non-cancer','Cancer']\n",
    "\n",
    "train_data = []\n",
    "for id, categ_name in enumerate(disease_types):\n",
    "    for file in os.listdir(os.path.join(train_dir, categ_name)):\n",
    "        train_data.append(['{}/{}'.format(categ_name, file), id, categ_name])\n",
    "train = pd.DataFrame(train_data, columns=['File', 'DiseaseID','Disease Type'])\n",
    "\n",
    "val_data = []\n",
    "for id, categ_name in enumerate(disease_types):\n",
    "    for file in os.listdir(os.path.join(val_dir, categ_name)):\n",
    "        val_data.append(['{}/{}'.format(categ_name, file), id, categ_name])\n",
    "val = pd.DataFrame(val_data, columns=['File', 'DiseaseID','Disease Type'])\n",
    "\n",
    "test_data = []\n",
    "for id, categ_name in enumerate(disease_types):\n",
    "    for file in os.listdir(os.path.join(test_dir, categ_name)):\n",
    "        test_data.append(['{}/{}'.format(categ_name, file), id, categ_name])\n",
    "test = pd.DataFrame(test_data, columns=['File', 'DiseaseID','Disease Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 299\n",
    "X_train = np.zeros((train.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "for i, file in tqdm(enumerate(train['File'].values)):\n",
    "    image = cv2.imread(os.path.join(train_dir, file))\n",
    "    if image is not None:\n",
    "        X_train[i] = cv2.resize(image.copy(), (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "X_val = np.zeros((val.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "for i, file in tqdm(enumerate(val['File'].values)):\n",
    "    image = cv2.imread(os.path.join(val_dir, file))\n",
    "    if image is not None:\n",
    "        X_val[i] = cv2.resize(image.copy(), (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "X_test = np.zeros((test.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "for i, file in tqdm(enumerate(test['File'].values)):\n",
    "    image = cv2.imread(os.path.join(test_dir, file))\n",
    "    if image is not None:\n",
    "        X_test[i] = cv2.resize(image.copy(), (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_val = X_val / 255.\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255.\n",
    "print('Train Shape: ', X_train.shape)\n",
    "print('Validation Shape: ', X_val.shape)\n",
    "print('Testing Shape: ', X_val.shape)\n",
    "\n",
    "Y_train = to_categorical(train['DiseaseID'].values, num_classes=2)\n",
    "Y_val = to_categorical(val['DiseaseID'].values, num_classes=2)\n",
    "Y_test = to_categorical(test['DiseaseID'].values, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 120\n",
    "\n",
    "def incep(fine_tune=94):\n",
    "\n",
    "    conv_base = InceptionV3(include_top=False, weights='imagenet', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    if fine_tune > 0:\n",
    "        for layer in conv_base.layers[:-fine_tune]:\n",
    "            layer.trainable = False\n",
    "    else:\n",
    "        for layer in conv_base.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    x = tf.keras.layers.UpSampling2D(size=(4, 4))(conv_base.output)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x) \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    out_layer = tf.keras.layers.Dense(2, activation='softmax', name='actiation')(x)\n",
    "\n",
    "    model = Model(inputs=conv_base.input, outputs=out_layer)\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-2)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(width_shift_range=0.2, height_shift_range=0.2, \n",
    "                                   zoom_range=0.2, shear_range=0.15,\n",
    "                                   horizontal_flip=True, vertical_flip=True, \n",
    "                                   rotation_range=10, fill_mode=\"nearest\")\n",
    "train_datagen.fit(X_train)\n",
    "validation_datagen = ImageDataGenerator()\n",
    "validation_datagen.fit(X_val)\n",
    "test_datagen = ImageDataGenerator()\n",
    "test_datagen.fit(X_test)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(filepath='weights.h5',\n",
    "                                      monitor='val_accuracy',\n",
    "                                      mode='max',\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = incep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
    "                 steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "                 epochs=EPOCHS, verbose=2,\n",
    "                 validation_data=(X_val, Y_val),\n",
    "                 callbacks=[checkpoint_callback]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss, final_accuracy = model.evaluate(X_val, Y_val)\n",
    "print(f'Final Loss: {final_loss}, Final Accuracy: {final_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('incep.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy plot\n",
    "figure(figsize=(6, 4))\n",
    "plt.plot(hist.history['accuracy'], color='#1338BE')\n",
    "plt.plot(hist.history['val_accuracy'], color='#B90E0A')\n",
    "plt.title(f'Accuracy', size=16)\n",
    "plt.ylabel('Accuracy', size=14)\n",
    "plt.xlabel('Epoch', size=14)\n",
    "plt.legend(['Train', 'Validation'], loc='lower right', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Loss plot\n",
    "plt.plot(hist.history['loss'], color='#1338BE')\n",
    "plt.plot(hist.history['val_loss'], color='#B90E0A')\n",
    "plt.title(f'Loss', size=16)\n",
    "plt.ylabel('Loss', size=14)\n",
    "plt.xlabel('Epoch', size=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(['Train', 'Validation'], loc='upper right', fontsize=14)\n",
    "#plt.ylim([0.05, 0.9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "Y_pred = np.argmax(Y_pred, axis=1)\n",
    "Y_true = np.argmax(Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_true, Y_pred)\n",
    "print(cm)\n",
    "print(confusion_matrix(Y_true, Y_pred))\n",
    "print(classification_report(Y_true, Y_pred))\n",
    "\n",
    "# Calculating metrics\n",
    "total=sum(sum(cm))\n",
    "accuracy=(cm[0,0]+cm[1,1])/total\n",
    "print ('Accuracy : ', accuracy)\n",
    "sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "print('Sensitivity : ', sensitivity )\n",
    "specificity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "print('Specificity : ', specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating AUC\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, Y_pred[:, 1])\n",
    "auc_ = auc(fpr, tpr)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label=f'{model} = {auc}')\n",
    "plt.ylabel('True positive rate', size=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title('ROC curve', size=16)\n",
    "plt.legend(loc='best', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
